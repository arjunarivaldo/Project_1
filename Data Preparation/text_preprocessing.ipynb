{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e73aeOAAxpA4"
   },
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6562,
     "status": "ok",
     "timestamp": 1760005611271,
     "user": {
      "displayName": "Arjuna Rivaldo",
      "userId": "01428253285493029486"
     },
     "user_tz": -540
    },
    "id": "M_O1n49Rxchi",
    "outputId": "b205524d-70bf-49da-b336-066410512eae"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import hstack # Diimpor tapi tidak digunakan dalam kode ini\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # Diimpor tapi tidak digunakan dalam kode ini\n",
    "from indo_normalizer import Normalizer\n",
    "from wordcloud import WordCloud\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# --- Download Data NLTK ---\n",
    "# Mengunduh resource NLTK yang diperlukan\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab') # Resource ini mungkin tidak standar atau diperlukan\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1734,
     "status": "ok",
     "timestamp": 1760005613009,
     "user": {
      "displayName": "Arjuna Rivaldo",
      "userId": "01428253285493029486"
     },
     "user_tz": -540
    },
    "id": "9iFOUtkxx7_C"
   },
   "outputs": [],
   "source": [
    "FILE_PATH = \"tweet.csv\"\n",
    "\n",
    "# Load Dataset dan pastikan semua kolom bertipe string\n",
    "df = pd.read_csv(FILE_PATH).astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEmeqjSzF0nj"
   },
   "source": [
    "# Cleaning Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 235457,
     "status": "ok",
     "timestamp": 1760005848482,
     "user": {
      "displayName": "Arjuna Rivaldo",
      "userId": "01428253285493029486"
     },
     "user_tz": -540
    },
    "id": "VJ3hNJkgiIxF",
    "outputId": "2d97e762-fbb4-4260-c75e-0fb4569aef4d"
   },
   "outputs": [],
   "source": [
    "# --- Text Normalizer ---\n",
    "normalizer = Normalizer()\n",
    "\n",
    "# Daftar stopwords standar dari NLTK\n",
    "stop_words_standard = set(stopwords.words('indonesian'))\n",
    "\n",
    "# Daftar kata-kata penting yang TIDAK BOLEH dihapus (pengecualian)\n",
    "list_stopwords_exceptions = [\n",
    "    'baik','maju','hebat','berhasil','kuat','setuju','mantap','yakin','terus','murah','adil','lancar',\n",
    "    'mulia','bangga','cemerlang','senang','keren','optimis','menang','sukses','terbukti','sejahtera',\n",
    "    'bagus','terbaik','solusi','amanah','jujur','peduli','tegas','pintar',\n",
    "    'tidak','bukan','jangan','belum','gagal','jelek','susah','mahal','rugi','hancur','lemah','salah',\n",
    "    'keliru','buruk','parah','bingung','mengecewakan','bohong','kacau','nyungsep','menjerit','terpuruk',\n",
    "    'turun','kurang','goblok','tolol'\n",
    "]\n",
    "\n",
    "list_stopwords_include = [\n",
    "    'jokowi', 'ekonomi', 'prabowosalahkansby', 'pilihorangbaik', 'pilihbajuputih',\n",
    "    'pilihjelasislamnya', 'jokowimenangtotaldebat'\n",
    "]\n",
    "custom_stopwords_exceptions = set(list_stopwords_exceptions)\n",
    "\n",
    "\n",
    "# --- DAFTAR STOPWORD FINAL ---\n",
    "\n",
    "# Hapus kata-kata pengecualian dari daftar gabungan. Ini adalah daftar final yang akan kita gunakan.\n",
    "final_stop_words = (stop_words_standard - custom_stopwords_exceptions) | set(list_stopwords_include)\n",
    "\n",
    "\n",
    "# --- BUAT SATU FUNGSI PREPROCESSING UTAMA ---\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Hapus karakter non-ASCII (Untuk mengatasi emoji dan mojibake)\n",
    "    text = text.encode('ascii', 'ignore').decode('utf-8')\n",
    "    # Hapus URL\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+|pic.twitter.com\\S+|t.co|S+', '', text, flags=re.MULTILINE)\n",
    "    # Hapus @username dan #hashtag\n",
    "    text = re.sub(r'\\@|#', '', text)\n",
    "    # Hapus tanda baca, angka, dan karakter spesial\n",
    "    text = re.sub(r'[^a-zA-Z\\s | ? ]', '', text)\n",
    "    # Normalize menggunakan indo_normalizer\n",
    "    normalized_tuple = normalizer.normalize_text(text)\n",
    "    text = normalized_tuple[0] # Extract the normalized text from the tuple\n",
    "\n",
    "\n",
    "    # Tokenisasi (memecah teks menjadi daftar kata)\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Menghapus stopwords menggunakan daftar final kita\n",
    "    tokens = [word for word in tokens if word not in final_stop_words]\n",
    "\n",
    "\n",
    "    # Menggabungkan kembali token menjadi string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "# --- Eksekusi pada DataFrame ---\n",
    "df['tweet_bersih']   = df['tweet'].apply(preprocess_text)\n",
    "\n",
    "print('Hasil setelah preprocessing')\n",
    "print(df[['tweet','sentimen','tweet_bersih']].head())\n",
    "\n",
    "print('\\n','='*50,'\\n')\n",
    "print('Statistik sentimen setelah preprocessing')\n",
    "print(df['sentimen'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcPFh-6bF0nl"
   },
   "source": [
    "# Visualisasi Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "executionInfo": {
     "elapsed": 3498,
     "status": "ok",
     "timestamp": 1760005851978,
     "user": {
      "displayName": "Arjuna Rivaldo",
      "userId": "01428253285493029486"
     },
     "user_tz": -540
    },
    "id": "TNzzAM3QF0nl",
    "outputId": "ee289948-c5a4-45f5-8e46-2cefb66759f5"
   },
   "outputs": [],
   "source": [
    "positif_text_updated = ' '.join(df[df['sentimen'] == 'positif']['tweet_bersih'])\n",
    "negatif_text_updated = ' '.join(df[df['sentimen'] == 'negatif']['tweet_bersih'])\n",
    "netral_text_updated = ' '.join(df[df['sentimen'] == 'netral']['tweet_bersih'])\n",
    "\n",
    "wordcloud_positif_upd = WordCloud(width=800, height=400, background_color='white').generate(positif_text_updated)\n",
    "wordcloud_negatif_upd = WordCloud(width=800, height=400, background_color='white').generate(negatif_text_updated)\n",
    "wordcloud_netral_upd = WordCloud(width=800, height=400, background_color='white').generate(netral_text_updated)\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(wordcloud_positif_upd, interpolation='bilinear')\n",
    "plt.title('Word Cloud untuk Sentimen Positif')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(wordcloud_negatif_upd, interpolation='bilinear')\n",
    "plt.title('Word Cloud untuk Sentimen Negatif')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(wordcloud_netral_upd, interpolation='bilinear')\n",
    "plt.title('Word Cloud untuk Sentimen Netral')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- Opsi save file --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1760005851988,
     "user": {
      "displayName": "Arjuna Rivaldo",
      "userId": "01428253285493029486"
     },
     "user_tz": -540
    },
    "id": "k9P6oiEUN2MJ"
   },
   "outputs": [],
   "source": [
    "# df[['sentimen', 'tweet_bersih']].to_csv('tweet_bersih.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
